#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Demo Script for Trained Frozen Lake Agent
è®­ç»ƒå¥½çš„å†°æ¹–æ™ºèƒ½ä½“æ¼”ç¤ºè„šæœ¬
"""

import numpy as np
import matplotlib.pyplot as plt
from frozen_lake_env import FrozenLakeWrapper
from q_learning_frozen_lake import QLearningAgent
import time


def compare_agents(map_size="4x4", slippery=True, model_path=None, num_episodes=20):
    """
    æ¯”è¾ƒè®­ç»ƒå‰åæ™ºèƒ½ä½“çš„è¡¨ç°

    å‚æ•°:
        map_size: åœ°å›¾å¤§å°
        slippery: æ˜¯å¦å…‰æ»‘
        model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
        num_episodes: å¯¹æ¯”å›åˆæ•°
    """
    print("ğŸ¯ Frozen Lake Agent Performance Comparison")
    print("=" * 60)

    # åˆ›å»ºç¯å¢ƒ
    env = FrozenLakeWrapper(map_size, slippery)

    # åŠ è½½è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    trained_agent = QLearningAgent(env.n_states, env.n_actions)
    if model_path:
        model_loaded = trained_agent.load_model(model_path)
    else:
        # å°è¯•è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹
        slippery_str = "slippery" if slippery else "non_slippery"
        default_path = f"frozen_lake_{map_size}_{slippery_str}_agent.pkl"
        model_loaded = trained_agent.load_model(default_path)

    if not model_loaded:
        print("âŒ No trained model found.")
        print("Please run training first or specify model path with --model")
        return

    # è®¾ç½®è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“ä¸ºæµ‹è¯•æ¨¡å¼ï¼ˆæ— æ¢ç´¢ï¼‰
    trained_agent.epsilon = 0.0

    # åˆ›å»ºéšæœºæ™ºèƒ½ä½“ä½œä¸ºå¯¹æ¯”
    random_agent = QLearningAgent(env.n_states, env.n_actions)
    random_agent.epsilon = 1.0  # æ€»æ˜¯éšæœºåŠ¨ä½œ

    results = {
        'trained': {'rewards': [], 'steps': [], 'paths': []},
        'random': {'rewards': [], 'steps': [], 'paths': []}
    }

    print(f"Running {num_episodes} episodes for each agent...")
    print("-" * 60)

    # æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    print("ğŸ¤– Testing Trained Agent:")
    for episode in range(num_episodes):
        state, info = env.reset()
        episode_reward = 0
        steps = 0
        path = [state]
        done = False

        while not done:
            action = trained_agent.get_action(state, training=False)
            state, reward, terminated, truncated, info = env.step(action)

            episode_reward += reward
            steps += 1
            path.append(state)
            done = terminated or truncated

            # é˜²æ­¢æ— é™å¾ªç¯
            if steps >= 100:
                done = True

        results['trained']['rewards'].append(episode_reward)
        results['trained']['steps'].append(steps)
        results['trained']['paths'].append(path)

        status = "SUCCESS âœ…" if episode_reward == 1.0 else "FAILED âŒ"
        print(f"  Episode {episode + 1:2d}: {status} | Steps: {steps:2d} | Path: {path}")

    print("\nğŸ² Testing Random Agent:")
    for episode in range(num_episodes):
        state, info = env.reset()
        episode_reward = 0
        steps = 0
        path = [state]
        done = False

        while not done:
            action = random_agent.get_action(state, training=True)  # æ€»æ˜¯éšæœº
            state, reward, terminated, truncated, info = env.step(action)

            episode_reward += reward
            steps += 1
            path.append(state)
            done = terminated or truncated

            # é˜²æ­¢æ— é™å¾ªç¯
            if steps >= 100:
                done = True

        results['random']['rewards'].append(episode_reward)
        results['random']['steps'].append(steps)
        results['random']['paths'].append(path)

        status = "SUCCESS âœ…" if episode_reward == 1.0 else "FAILED âŒ"
        print(f"  Episode {episode + 1:2d}: {status} | Steps: {steps:2d} | Path: {path}")

    env.close()

    # åˆ†æç»“æœ
    analyze_results(results, map_size, slippery)

    # å¯è§†åŒ–ç»“æœ
    plot_comparison(results, map_size, slippery)


def analyze_results(results, map_size, slippery):
    """åˆ†ææ¯”è¾ƒç»“æœ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š PERFORMANCE ANALYSIS")
    print("=" * 60)

    for agent_name, data in results.items():
        rewards = data['rewards']
        steps = data['steps']

        success_count = sum(1 for r in rewards if r == 1.0)
        success_rate = success_count / len(rewards) * 100

        print(f"\n{agent_name.upper()} AGENT ({map_size}, {'slippery' if slippery else 'non-slippery'}):")
        print(f"  Success Rate: {success_rate:.1f}% ({success_count}/{len(rewards)})")
        print(f"  Average Steps: {np.mean(steps):.1f} Â± {np.std(steps):.1f}")
        print(f"  Min Steps: {np.min(steps)}")
        print(f"  Max Steps: {np.max(steps)}")

        # è·¯å¾„é•¿åº¦åˆ†æ
        path_lengths = [len(path) - 1 for path in data['paths']]  # -1 å› ä¸ºåŒ…å«èµ·ç‚¹
        print(f"  Average Path Length: {np.mean(path_lengths):.1f}")

    # è®¡ç®—æ”¹è¿›ç¨‹åº¦
    trained_success = sum(1 for r in results['trained']['rewards'] if r == 1.0) / len(results['trained']['rewards'])
    random_success = sum(1 for r in results['random']['rewards'] if r == 1.0) / len(results['random']['rewards'])

    if random_success > 0:
        success_improvement = ((trained_success - random_success) / random_success) * 100
        print("\nğŸ¯ IMPROVEMENT:")
        print(f"  Success Rate Improvement: {success_improvement:+.1f}%")
    else:
        print("\nğŸ¯ RESULT:")
        print(f"  Random agent success rate: 0.0%")
        print(f"  Trained agent success rate: {trained_success*100:.1f}%")

    if trained_success > 0.5:
        print("  âœ… Trained agent performs VERY WELL!")
    elif trained_success > 0.2:
        print("  ğŸŸ¡ Trained agent performs reasonably well.")
    else:
        print("  âŒ Trained agent needs more training.")


def plot_comparison(results, map_size, slippery):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    slippery_str = "slippery" if slippery else "non-slippery"
    fig.suptitle(f'Frozen Lake {map_size} {slippery_str.capitalize()} - Agent Comparison',
                 fontsize=16, fontweight='bold')

    # æˆåŠŸç‡å¯¹æ¯”
    trained_success = sum(1 for r in results['trained']['rewards'] if r == 1.0) / len(results['trained']['rewards'])
    random_success = sum(1 for r in results['random']['rewards'] if r == 1.0) / len(results['random']['rewards'])

    bars = axes[0, 0].bar(['Trained Agent', 'Random Agent'],
                          [trained_success * 100, random_success * 100],
                          color=['blue', 'red'], alpha=0.7, width=0.6)
    axes[0, 0].set_ylabel('Success Rate (%)')
    axes[0, 0].set_title('Success Rate Comparison')
    axes[0, 0].set_ylim(0, 100)
    axes[0, 0].grid(True, alpha=0.3, axis='y')

    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        height = bar.get_height()
        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 1,
                       '.1f', ha='center', va='bottom', fontweight='bold')

    # æ­¥æ•°åˆ†å¸ƒ
    axes[0, 1].hist(results['trained']['steps'], bins=10, alpha=0.7,
                    label='Trained', color='blue')
    axes[0, 1].hist(results['random']['steps'], bins=10, alpha=0.7,
                    label='Random', color='red')
    axes[0, 1].set_xlabel('Steps per Episode')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].set_title('Steps Distribution')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3, axis='y')

    # å¥–åŠ±æ›²çº¿
    episodes = range(1, len(results['trained']['rewards']) + 1)
    axes[1, 0].plot(episodes, np.cumsum(results['trained']['rewards']) / episodes,
                    'b-', linewidth=2, label='Trained (Cumulative)')
    axes[1, 0].plot(episodes, np.cumsum(results['random']['rewards']) / episodes,
                    'r-', linewidth=2, label='Random (Cumulative)')
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Average Reward')
    axes[1, 0].set_title('Learning Curve (Cumulative Average)')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].set_ylim(0, 1)

    # è·¯å¾„é•¿åº¦å¯¹æ¯”
    trained_path_lengths = [len(path) - 1 for path in results['trained']['paths']]
    random_path_lengths = [len(path) - 1 for path in results['random']['paths']]

    axes[1, 1].boxplot([trained_path_lengths, random_path_lengths],
                       labels=['Trained Agent', 'Random Agent'])
    axes[1, 1].set_ylabel('Path Length')
    axes[1, 1].set_title('Path Length Distribution')
    axes[1, 1].grid(True, alpha=0.3, axis='y')

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    slippery_str = "slippery" if slippery else "non_slippery"
    filename = f"frozen_lake_{map_size}_{slippery_str}_demo_results.png"
    plt.savefig(filename, dpi=200, bbox_inches='tight')
    print(f"\nDemo plot saved as: {filename}")

    plt.show()


def interactive_demo(model_path=None, map_size="4x4", slippery=True, max_steps=50):
    """äº¤äº’å¼æ¼”ç¤º"""
    print("ğŸ® Interactive Frozen Lake Demo")
    print("=" * 40)

    env = FrozenLakeWrapper(map_size, slippery)

    # åŠ è½½è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    agent = QLearningAgent(env.n_states, env.n_actions)
    if model_path:
        if not agent.load_model(model_path):
            print("âŒ No trained model found.")
            return
    else:
        # å°è¯•è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹
        slippery_str = "slippery" if slippery else "non_slippery"
        default_path = f"frozen_lake_{map_size}_{slippery_str}_agent.pkl"
        if not agent.load_model(default_path):
            print("âŒ No trained model found.")
            print("Please run training first or specify model path with --model")
            return

    agent.epsilon = 0.0  # æ— æ¢ç´¢

    print("ğŸ¤– Trained agent is ready!")
    print("ğŸ¯ Watch the agent navigate the frozen lake...")
    print("Map legend: ğŸ =Start, ğŸ†=Goal, ğŸ•³ï¸=Hole, ğŸ§Š=Ice, ğŸ¤–=Agent")
    print("-" * 60)

    # è¿è¡Œæ¼”ç¤º
    state, info = env.reset()
    total_reward = 0
    steps = 0
    path = [state]

    env.render_ascii()
    print(f"Start position: State {state}")
    print("-" * 60)

    while steps < max_steps:
        # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
        action = agent.get_action(state, training=False)
        action_names = ['LEFT â†', 'DOWN â†“', 'RIGHT â†’', 'UP â†‘']

        print(f"Step {steps + 1}: Choosing action {action_names[action]}")

        # æ‰§è¡ŒåŠ¨ä½œ
        state, reward, terminated, truncated, info = env.step(action)

        total_reward += reward
        steps += 1
        path.append(state)

        # æ˜¾ç¤ºç»“æœ
        env.render_ascii()
        print(f"  New position: State {state}, Reward: {reward}")
        print(f"  Total reward: {total_reward}")
        print("-" * 60)

        if terminated or truncated:
            if reward == 1.0:
                print("ğŸ‰ SUCCESS! Agent reached the goal!")
            elif terminated:
                print("ğŸ’¥ FAILED! Agent fell into a hole!")
            else:
                print("â° Episode truncated (max steps reached)")
            break

        time.sleep(0.5)  # çŸ­æš‚æš‚åœä»¥è§‚å¯Ÿ

    print("\nğŸ¯ DEMO COMPLETED!")
    print(f"Final reward: {total_reward}")
    print(f"Total steps: {steps}")
    print(f"Path taken: {path}")

    if total_reward == 1.0:
        print("âœ… Agent successfully navigated the frozen lake!")
    else:
        print("âŒ Agent failed to reach the goal.")

    env.close()


def show_policy_demo(model_path=None, map_size="4x4", slippery=True):
    """æ˜¾ç¤ºå­¦ä¹ åˆ°çš„ç­–ç•¥"""
    print("ğŸ§  Learned Policy Visualization")
    print("=" * 40)

    env = FrozenLakeWrapper(map_size, slippery)

    # åŠ è½½è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    agent = QLearningAgent(env.n_states, env.n_actions)
    if model_path:
        if not agent.load_model(model_path):
            return
    else:
        slippery_str = "slippery" if slippery else "non_slippery"
        default_path = f"frozen_lake_{map_size}_{slippery_str}_agent.pkl"
        if not agent.load_model(default_path):
            return

    policy = agent.get_optimal_policy()
    state_values = agent.get_state_values()

    print(f"Environment: {map_size} {'slippery' if slippery else 'non-slippery'}")
    print("\nLearned Policy (Optimal Actions):")
    print("Legend: â†=Left, â†“=Down, â†’=Right, â†‘=Up")
    print("-" * (env.map_size * 4 + 1))

    action_symbols = ['â†', 'â†“', 'â†’', 'â†‘']
    for i in range(env.map_size):
        for j in range(env.map_size):
            state = i * env.map_size + j
            cell_type = env.desc[i, j]

            if cell_type == 'S':
                symbol = 'ğŸ '  # èµ·ç‚¹
            elif cell_type == 'G':
                symbol = 'ğŸ†'  # ç›®æ ‡
            elif cell_type == 'H':
                symbol = 'ğŸ•³ï¸'  # æ´
            else:
                symbol = action_symbols[policy[state]]

            print(f" {symbol} ", end="")
        print()
    print("-" * (env.map_size * 4 + 1))

    print("\nState Values (Q-max):")
    print("-" * (env.map_size * 8 + 1))
    for i in range(env.map_size):
        for j in range(env.map_size):
            state = i * env.map_size + j
            value = state_values[state]
            print("5.2f", end=" ")
        print()
    print("-" * (env.map_size * 8 + 1))

    # ç»Ÿè®¡ä¿¡æ¯
    stats = agent.get_stats()
    print("\nğŸ“Š Model Statistics:")
    print(f"  Q-table shape: {stats['q_table_shape']}")
    print(f"  Max Q-value: {stats['max_q_value']:.3f}")
    print(f"  Min Q-value: {stats['min_q_value']:.3f}")
    print(f"  Explored states: {stats['explored_states']}/{stats['q_table_shape'][0]}")
    print(".1f")
    env.close()


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Demo trained Frozen Lake agent')
    parser.add_argument('--map', type=str, default='4x4', choices=['4x4', '8x8'],
                       help='Map size')
    parser.add_argument('--slippery', action='store_true', default=True,
                       help='Use slippery ice (default: True)')
    parser.add_argument('--no-slippery', action='store_true',
                       help='Use non-slippery ice')
    parser.add_argument('--model', type=str, default=None,
                       help='Path to trained model')
    parser.add_argument('--episodes', type=int, default=20,
                       help='Number of comparison episodes')
    parser.add_argument('--interactive', action='store_true',
                       help='Run interactive demo')
    parser.add_argument('--policy', action='store_true',
                       help='Show learned policy visualization')

    args = parser.parse_args()

    if args.no_slippery:
        args.slippery = False

    if args.policy:
        show_policy_demo(args.model, args.map, args.slippery)
    elif args.interactive:
        interactive_demo(args.model, args.map, args.slippery)
    else:
        compare_agents(args.map, args.slippery, args.model, args.episodes)


if __name__ == "__main__":
    main()
