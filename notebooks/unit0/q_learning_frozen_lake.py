#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Q-Learning Agent for Frozen Lake
å†°æ¹–æ¸¸æˆçš„Q-Learningæ™ºèƒ½ä½“å®ç°
"""

import numpy as np
import pickle
import os


class QLearningAgent:
    """Q-Learningæ™ºèƒ½ä½“"""

    def __init__(self, n_states, n_actions,
                 learning_rate=0.1, discount_factor=0.99, epsilon=1.0,
                 epsilon_decay=0.9995, epsilon_min=0.01):
        """
        åˆå§‹åŒ–Q-Learningæ™ºèƒ½ä½“

        å‚æ•°:
            n_states: çŠ¶æ€æ•°é‡
            n_actions: åŠ¨ä½œæ•°é‡
            learning_rate: å­¦ä¹ ç‡
            discount_factor: æŠ˜æ‰£å› å­
            epsilon: æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
        """
        self.n_states = n_states
        self.n_actions = n_actions

        # Q-Learningå‚æ•°
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # åˆå§‹åŒ–Qè¡¨
        self.q_table = np.zeros((n_states, n_actions))

        print(f"Q-Learning Agent initialized:")
        print(f"  States: {n_states}, Actions: {n_actions}")
        print(f"  Learning rate: {self.learning_rate}")
        print(f"  Discount factor: {self.discount_factor}")
        print(f"  Initial epsilon: {self.epsilon}")

    def get_action(self, state, training=True):
        """
        æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œ

        å‚æ•°:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼

        è¿”å›:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        if training and np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return np.random.randint(self.n_actions)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            return np.argmax(self.q_table[state])

    def update_q_table(self, state, action, reward, next_state, done):
        """
        æ›´æ–°Qè¡¨

        å‚æ•°:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
        """
        # Q-Learningæ›´æ–°å…¬å¼
        current_q = self.q_table[state, action]

        if done:
            # ç»ˆæ­¢çŠ¶æ€ï¼Œæ²¡æœ‰æœªæ¥å¥–åŠ±
            target = reward
        else:
            # éç»ˆæ­¢çŠ¶æ€ï¼Œä½¿ç”¨Bellmanæ–¹ç¨‹
            max_future_q = np.max(self.q_table[next_state])
            target = reward + self.discount_factor * max_future_q

        # æ›´æ–°Qå€¼
        self.q_table[state, action] += self.learning_rate * (target - current_q)

    def decay_epsilon(self):
        """è¡°å‡æ¢ç´¢ç‡"""
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def save_model(self, filename):
        """ä¿å­˜æ¨¡å‹"""
        data = {
            'q_table': self.q_table,
            'epsilon': self.epsilon,
            'learning_rate': self.learning_rate,
            'discount_factor': self.discount_factor,
            'n_states': self.n_states,
            'n_actions': self.n_actions
        }

        with open(filename, 'wb') as f:
            pickle.dump(data, f)

        print(f"Model saved to {filename}")
        print(f"Q-table shape: {self.q_table.shape}")

    def load_model(self, filename):
        """åŠ è½½æ¨¡å‹"""
        if os.path.exists(filename):
            with open(filename, 'rb') as f:
                data = pickle.load(f)

            self.q_table = data['q_table']
            self.epsilon = data['epsilon']
            self.learning_rate = data['learning_rate']
            self.discount_factor = data['discount_factor']
            self.n_states = data['n_states']
            self.n_actions = data['n_actions']

            print(f"Model loaded from {filename}")
            print(f"Q-table shape: {self.q_table.shape}")
            return True
        else:
            print(f"Model file {filename} not found")
            return False

    def get_optimal_policy(self):
        """è·å–æœ€ä¼˜ç­–ç•¥"""
        policy = np.zeros(self.n_states, dtype=int)
        for state in range(self.n_states):
            policy[state] = np.argmax(self.q_table[state])
        return policy

    def get_state_values(self):
        """è·å–çŠ¶æ€å€¼å‡½æ•°"""
        return np.max(self.q_table, axis=1)

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'q_table_shape': self.q_table.shape,
            'total_q_values': self.q_table.size,
            'max_q_value': np.max(self.q_table),
            'min_q_value': np.min(self.q_table),
            'avg_q_value': np.mean(self.q_table),
            'epsilon': self.epsilon,
            'explored_states': np.count_nonzero(np.max(self.q_table, axis=1))
        }


def train_q_learning(env, episodes=5000, save_path="frozen_lake_agent.pkl"):
    """
    è®­ç»ƒQ-Learningæ™ºèƒ½ä½“

    å‚æ•°:
        env: ç¯å¢ƒå¯¹è±¡
        episodes: è®­ç»ƒå›åˆæ•°
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„

    è¿”å›:
        è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“å’Œå¥–åŠ±å†å²
    """
    print("ğŸš€ Starting Q-Learning Training")
    print("=" * 50)

    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = QLearningAgent(env.n_states, env.n_actions)

    # è®­ç»ƒç»Ÿè®¡
    rewards_history = []
    steps_history = []
    epsilon_history = []
    success_count = 0

    print(f"Training for {episodes} episodes...")
    print("-" * 50)

    for episode in range(episodes):
        state, info = env.reset()
        total_reward = 0
        steps = 0
        done = False

        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.get_action(state, training=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(action)

            # æ›´æ–°Qè¡¨
            done = terminated or truncated
            agent.update_q_table(state, action, reward, next_state, done)

            total_reward += reward
            steps += 1
            state = next_state

            # é˜²æ­¢æ— é™å¾ªç¯
            if steps >= 100:
                done = True

        # è¡°å‡æ¢ç´¢ç‡
        agent.decay_epsilon()

        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        rewards_history.append(total_reward)
        steps_history.append(steps)
        epsilon_history.append(agent.epsilon)

        # è®°å½•æˆåŠŸæ¬¡æ•°
        if total_reward == 1.0:
            success_count += 1

        # æ¯500å›åˆæ˜¾ç¤ºè¿›åº¦
        if (episode + 1) % 500 == 0:
            recent_success_rate = np.mean(rewards_history[-500:]) * 100
            avg_reward = np.mean(rewards_history[-500:])
            print(f"Episode {episode + 1:5d}/{episodes} | "
                  f"Avg Reward: {avg_reward:.3f} | "
                  f"Success Rate: {recent_success_rate:.1f}% | "
                  f"Epsilon: {agent.epsilon:.4f}")

    print("\n" + "=" * 50)
    print("ğŸ¯ TRAINING COMPLETED!")
    print("=" * 50)
    print(f"Final success rate (last 500): {np.mean(rewards_history[-500:]) * 100:.1f}%")
    print(f"Total successful episodes: {success_count}/{episodes}")
    print(f"Overall success rate: {success_count/episodes * 100:.1f}%")

    # ä¿å­˜æ¨¡å‹
    agent.save_model(save_path)

    return agent, rewards_history, steps_history, epsilon_history


# æµ‹è¯•Q-Learningæ™ºèƒ½ä½“
if __name__ == "__main__":
    from frozen_lake_env import FrozenLakeWrapper

    print("Testing Q-Learning Agent...")

    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = FrozenLakeWrapper("4x4", is_slippery=True)
    agent = QLearningAgent(env.n_states, env.n_actions)

    # æµ‹è¯•å‡ ä¸ªå›åˆçš„å­¦ä¹ 
    print("\nTraining test (100 episodes):")
    agent, rewards, steps, epsilons = train_q_learning(env, episodes=100)

    # æ˜¾ç¤ºæœ€ç»ˆQè¡¨
    print("\nFinal Q-table (first 5 states):")
    print(agent.q_table[:5])

    # æ˜¾ç¤ºæœ€ä¼˜ç­–ç•¥
    policy = agent.get_optimal_policy()
    action_names = ['LEFT', 'DOWN', 'RIGHT', 'UP']
    print("\nOptimal policy:")
    for i in range(env.n_states):
        row, col = i // env.map_size, i % env.map_size
        print(f"State {i:2d} ({row},{col}): {action_names[policy[i]]}")

    env.close()

    print("\nQ-Learning test completed!")
